    async def process_message_stream(self, message: str, session_id: str) -> str:
        """
        Procesa un mensaje a trav√©s del workflow LangGraph.
        
        NOTA: Usa el mismo flujo que process_message pero con logging mejorado.
        El streaming interno del LLM ya est√° habilitado en __init__.

        Args:
            message: Mensaje del usuario (puede ser m√∫ltiples mensajes unidos)
            session_id: ID de sesi√≥n √∫nico

        Returns:
            Respuesta completa del sistema
        """
        try:
            logger.info(f"üöÄ Procesando mensaje con streaming para sesi√≥n {session_id}")
            
            # Estado inicial
            initial_state: AgentState = {
                "messages": [HumanMessage(content=message)],
                "next": "authentication",
                "user_info": {},
                "session_id": session_id,
                "agent_scratchpad": {},
                "escalation_requested": False,
                "confidence_score": 1.0
            }

            # Configuraci√≥n con LangSmith tags
            config = {
                "configurable": {"thread_id": session_id},
                "recursion_limit": 10,
                "tags": [
                    f"session:{session_id}",
                    "streaming:enabled",
                    "source:whatsapp"
                ],
                "metadata": {
                    "session_id": session_id,
                    "message_preview": message[:100],
                    "timestamp": datetime.now().isoformat()
                }
            }

            # Ejecutar workflow normalmente (el streaming del LLM est√° habilitado internamente)
            result = await self.app.ainvoke(initial_state, config)

            # Extraer la respuesta del agente
            ai_messages = [msg for msg in result["messages"] if isinstance(msg, AIMessage)]
            
            if ai_messages:
                # Buscar la respuesta m√°s larga (la del agente, no solo metadatos)
                full_response = ""
                for msg in reversed(ai_messages):
                    if msg.content and len(msg.content) > 20:
                        full_response = msg.content
                        logger.info(f"‚úÖ Respuesta encontrada: {len(full_response)} caracteres")
                        break
                
                # Si no encontramos nada largo, usar el √∫ltimo
                if not full_response and ai_messages:
                    full_response = ai_messages[-1].content
                    logger.warning(f"‚ö†Ô∏è Usando √∫ltimo mensaje aunque es corto: '{full_response}'")
                
                if full_response:
                    return full_response
                else:
                    logger.error("‚ùå No se encontr√≥ respuesta v√°lida en ai_messages")
                    return "Lo siento, hubo un problema procesando tu mensaje."
            else:
                logger.error("‚ùå No hay mensajes de IA en el resultado")
                return "Lo siento, hubo un problema procesando tu mensaje."

        except Exception as e:
            logger.error(f"‚ùå Error en SupervisorAgent.process_message_stream: {e}", exc_info=True)
            return "Hubo un error t√©cnico. Por favor intent√° de nuevo."
