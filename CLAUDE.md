# ğŸ“ Universidad Austral Bot - GuÃ­a de Proyecto

## ğŸ“‹ DescripciÃ³n General

Bot universitario con arquitectura LangGraph + FastAPI + Supabase para atenciÃ³n estudiantil vÃ­a WhatsApp.

**Stack TecnolÃ³gico:**
- **Backend**: Python 3.11, FastAPI, LangGraph
- **Database**: Supabase (PostgreSQL)
- **LLM**: OpenAI GPT-4o-mini, Claude Opus 4.1
- **Integration**: Chatwoot (WhatsApp), n8n (tools)
- **Deployment**: Docker Compose

## ğŸ—ï¸ Arquitectura del Sistema

### Componentes Principales
1. **Supervisor Agent** (LangGraph): Orquesta 4 sub-agents especializados
2. **Academic Agent**: Horarios, inscripciones, profesores
3. **Calendar Agent**: ExÃ¡menes, fechas, calendario acadÃ©mico
4. **Financial Agent**: Pagos, deudas (futuro)
5. **Policies Agent**: Reglamentos, syllabi (futuro)

### Flujo de Datos
```
WhatsApp â†’ Chatwoot Webhook â†’ FastAPI â†’ LangGraph Supervisor
    â†“
Clasificador de Query â†’ Agent Especializado â†’ n8n Tool â†’ Supabase
    â†“
Response â†’ FastAPI â†’ Chatwoot â†’ WhatsApp
```

## ğŸ“‚ Estructura de Directorios

```
app/
â”œâ”€â”€ core/              # Config, constants, exceptions
â”œâ”€â”€ models/            # Pydantic models (CRÃTICO: usar properties)
â”œâ”€â”€ agents/            # LangGraph agents
â”œâ”€â”€ tools/             # n8n tool wrappers
â”œâ”€â”€ database/          # Supabase repositories
â”œâ”€â”€ integrations/      # Chatwoot, WhatsApp webhooks
â””â”€â”€ utils/             # Logging, formatters
```

## ğŸ› ï¸ Comandos Importantes

### Docker
```bash
# Rebuild completo
docker-compose down && docker-compose build --no-cache && docker-compose up -d

# Ver logs
docker-compose logs -f university-agent

# Restart solo el bot
docker-compose restart university-agent
```

### Testing
```bash
# Tests locales
pytest tests/ -v

# Test especÃ­fico
pytest tests/unit/test_agents.py -v
```

### Git
```bash
# Branch naming: feature/nombre-funcionalidad
# Commits: usar conventional commits (feat:, fix:, refactor:)
```

## ğŸ“ Convenciones de CÃ³digo

### Python Style
- **Type hints**: SIEMPRE usar (mandatory)
- **Pydantic Models**: Preferir sobre dicts
- **Properties calculadas**: Usar @property en models
- **Docstrings**: Google style
- **Naming**:
  - Variables: snake_case
  - Classes: PascalCase
  - Constants: UPPER_CASE
  - Private: _leading_underscore

### Importaciones
```python
# Orden: stdlib â†’ third-party â†’ local
import os
from typing import Dict, Any

from fastapi import FastAPI
from langchain_core.messages import HumanMessage

from app.core import settings
from app.models import HorariosResponse
```

### Logs
```python
# Usar logger, no print
from app.utils.logger import get_logger
logger = get_logger(__name__)

logger.info(f"Query clasificada como: {query_type}")
logger.error(f"Error: {e}", exc_info=True)
```

## âš ï¸ Reglas CrÃ­ticas

### NUNCA:
- âŒ Usar `dict` genÃ©ricos - usar Pydantic Models
- âŒ Hacer cambios en `main.py` sin consultar (critical)
- âŒ Modificar `.env` en commits (usar .env.example)
- âŒ Hardcodear URLs o API keys
- âŒ Usar `print()` - usar `logger`

### SIEMPRE:
- âœ… Validar UUIDs antes de queries a DB
- âœ… Usar try/except en tool calls
- âœ… Retornar Pydantic models desde repositories
- âœ… Usar properties calculadas en models
- âœ… Formatear responses con emojis (1-2 por mensaje)

## ğŸ”§ ConfiguraciÃ³n de Herramientas

### Filtrado de Contexto de Mensajes âš¡ NUEVO
**Problema solucionado**: Reducir consumo de tokens enviando solo contexto relevante

**ConfiguraciÃ³n** (`.env`):
```bash
MESSAGE_HISTORY_HOURS=24  # Solo mensajes de Ãºltimas N horas
```

**CÃ³mo funciona**:
- `supervisor.py:_get_filtered_message_history()` filtra mensajes por timestamp
- Solo incluye mensajes de las Ãºltimas 24 horas (configurable)
- Reduce significativamente el consumo de tokens
- Mantiene contexto relevante sin informaciÃ³n obsoleta

**Logs**:
```
ğŸ“Š Historial filtrado para {session_id}:
   {N} mensajes Ãºnicos de {M}/{T} estados
   (Ãºltimas 24h desde {timestamp})
```

### n8n Webhooks
Base URL: `https://n8n.tucbbs.com.ar/webhook`

Tools disponibles:
- `consultar_horarios`
- `ver_inscripciones`
- `buscar_profesor`
- `consultar_aula`
- `estado_cuenta`
- `consultar_creditos_vu`
- `calendario_academico`
- `consultar_examenes`
- `escalar_consulta`

### Supabase
```python
# Usar repositories, NUNCA queries directas
from app.database.academic_repository import AcademicRepository

repo = AcademicRepository()
response = await repo.get_horarios_alumno(request)
```

## ğŸ¨ TerminologÃ­a Universitaria Argentina

- "Cursada" (no "curso")
- "Materia" (no "asignatura")
- "ComisiÃ³n" (para grupos)
- "Final" / "Parcial" (exÃ¡menes)
- "Cuatrimestre" (no "semestre")
- "Carrera" (programa)
- Usar "vos" en respuestas

## ğŸ› Debugging

### Logs de LangSmith
- Project: `universidad-austral-bot`
- Ver traces en: https://smith.langchain.com

### Common Issues
1. **ValidationError en Pydantic**: Revisar types en Request
2. **Tool timeout**: n8n puede tardar >30s
3. **Context window full**: Usar properties en vez de full objects

## ğŸ“Š Performance

- **Max response time**: 5 segundos
- **Token budget**: Usar fuzzy matching antes de LLM
- **Cache**: Memory MCP mantiene contexto

## ğŸ¯ Workflow Recomendado

1. **Plan**: Leer cÃ³digo existente, entender arquitectura
2. **Research**: Buscar patrones similares en el cÃ³digo
3. **Implement**: Crear/modificar cÃ³digo
4. **Test**: Verificar que funciona
5. **Review**: Check logs de LangSmith

## ğŸ“š Recursos

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **Pydantic Docs**: https://docs.pydantic.dev/
- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **Supabase Docs**: https://supabase.com/docs

---

## ğŸ¤– Sistema de Respuestas LLM (NUEVO - Octubre 2025)

### ğŸ¯ Objetivo

Transformar el bot de respuestas basadas en templates rÃ­gidos a **respuestas generadas por LLM** que sean:
- Naturales y conversacionales
- Contextuales (usan historial de 24h)
- Selectivas (responden solo lo preguntado)
- EmpÃ¡ticas (adaptan tono segÃºn emociÃ³n/urgencia)
- Proactivas (sugieren informaciÃ³n Ãºtil relacionada)

### ğŸ“Š Arquitectura

**Flujo anterior (templates)**:
```
Usuario â†’ Agent â†’ Tool (datos) â†’ Template Formatter â†’ Usuario
```

**Flujo nuevo (LLM)**:
```
Usuario â†’ Agent â†’ Tool (datos) â†’ LLM Response Generator â†’ Usuario
                                         â†‘
                                 Context Enhancer
                                 (historial 24h + emociones + proactividad)
```

### ğŸ“ Nuevos MÃ³dulos

#### 1. `app/models/context.py`
Modelos Pydantic para contexto conversacional enriquecido:
- `EmotionalState` - Estado emocional (tono, urgencia, empatÃ­a)
- `QueryEntity` - Entidades extraÃ­das (aula, horario, profesor)
- `ConversationContext` - Contexto completo (historial, emociones, sugerencias)
- `ResponseStrategy` - Estrategia de respuesta (longitud, tono, formato)

#### 2. `app/prompts/system_prompts.py`
System prompts estructurados para el LLM:
- Prompts base por agent (academic, calendar)
- Instrucciones de tono emocional (urgente, celebraciÃ³n, empatÃ­a, Ã¡nimo)
- Instrucciones de longitud (short, medium, detailed, auto)
- **Instrucciones de selectividad** (CRÃTICO: responder solo lo preguntado)
- Instrucciones de proactividad y referencias histÃ³ricas

#### 3. `app/utils/llm_response_generator.py`
Generador principal de respuestas con LLM:
- `LLMResponseGenerator` class - Generador principal
- `generate_natural_response()` - Helper function para uso fÃ¡cil
- `should_use_llm_generation()` - Determina si usar LLM vs templates
- `determine_data_complexity()` - Analiza complejidad de datos

#### 4. `app/utils/response_strategy.py`
Analiza queries y determina estrategia de respuesta:
- `QueryEntityExtractor` - Extrae entidades (aula, horario, profesor)
- `ResponseStrategyBuilder` - Construye estrategia completa
- `build_response_strategy()` - Helper function

#### 5. `app/utils/context_enhancer.py`
Enriquece contexto conversacional:
- `ContextEnhancer` class - Enriquecedor principal
- Extrae historial relevante (Ãºltimas 24h)
- Detecta estado emocional
- Genera sugerencias proactivas
- `enhance_conversation_context()` - Helper function

### âš™ï¸ ConfiguraciÃ³n (.env)

```bash
# LLM Response Generation
RESPONSE_GENERATION_MODE=llm  # "llm", "template", "hybrid"
LLM_RESPONSE_MODEL=gpt-4o-mini  # Opcional, usa LLM_MODEL si no se especifica
LLM_RESPONSE_TEMPERATURE=0.5  # Balance creatividad/precisiÃ³n
MAX_RESPONSE_TOKENS=500

# Context Enhancement
ENABLE_CONTEXT_ENHANCEMENT=true
CONTEXT_LOOKBACK_HOURS=24  # Historial conversacional
ENABLE_PROACTIVE_SUGGESTIONS=true

# Response Strategy
ENABLE_SMART_FILTERING=true  # LLM filtra datos relevantes
DEFAULT_RESPONSE_LENGTH=auto  # "short", "medium", "detailed", "auto"
```

### ğŸ”„ IntegraciÃ³n en Agents

**Ejemplo en `academic_agent.py`** (mÃ©todo `_handle_schedules`):

```python
# Obtener datos del tool
result_dict = await self.tools.consultar_horarios(params)
response = HorariosResponse(**result_dict)

# Verificar si usar LLM generation
if should_use_llm_generation():
    # Obtener sesiÃ³n
    session = session_manager.get_session(session_id)

    # Enriquecer contexto
    context = await enhance_conversation_context(
        current_query=query,
        query_type="horarios",
        user_name=user_info["nombre"],
        session=session,
        data=response
    )

    # Construir estrategia
    strategy, entities = build_response_strategy(
        query=query,
        data=response,
        context=context
    )

    # Generar respuesta natural
    natural_response = await generate_natural_response(
        data=response,
        original_query=query,
        user_name=user_info["nombre"],
        query_type="horarios",
        agent_type="academic",
        context=context,
        strategy=strategy
    )

    # Actualizar contexto en sesiÃ³n
    session.update_query_context(
        query=query,
        query_type="horarios",
        query_data={"materia": materia, "temporal": contexto_temporal},
        response_summary=natural_response[:100]
    )

    return natural_response
else:
    # Fallback a templates
    return self._format_schedule_response(response, nombre, contexto_temporal)
```

### ğŸ“Š ComparaciÃ³n de Respuestas

**Pregunta**: "Â¿En quÃ© aula tengo Ã©tica?"

**Antes (template rÃ­gido)**:
```
ğŸ“š Horarios de Juan

â€¢ Ã‰tica y DeontologÃ­a (14:00 - 16:00)
  ğŸ“ Aula R3 ğŸ«
  ğŸ‘¨â€ğŸ« Prof. GarcÃ­a MartÃ­nez
  â±ï¸ DuraciÃ³n: 120 minutos

Â¿Algo mÃ¡s? ğŸ¤
```

**DespuÃ©s (LLM selectivo)**:
```
Tu clase de Ã‰tica es en el aula R3 ğŸ“

(Es los viernes a las 14hs con GarcÃ­a MartÃ­nez)
```

**Pregunta**: "Â¿CuÃ¡ndo tengo clases maÃ±ana?"

**DespuÃ©s (LLM con proactividad)**:
```
MaÃ±ana tenÃ©s un dÃ­a bastante cargado con 3 materias ğŸ’ª

14:00 - Ã‰tica (R3)
16:00 - ProgramaciÃ³n I (L2)
18:30 - MatemÃ¡tica Discreta (R1)

Ah, y acordate que tenÃ©s el parcial de Nativa Digital el lunes ğŸŸ¡
Â¿QuerÃ©s que te recuerde el aula?
```

### âœ… Testing

Ejecutar test de validaciÃ³n:
```bash
python test_llm_response_system.py
```

El test valida:
1. âœ… Imports de mÃ³dulos nuevos
2. âœ… Configuraciones cargadas correctamente
3. âœ… Modelos Pydantic funcionando
4. âœ… Extractores de entidades
5. âœ… GeneraciÃ³n de prompts
6. âœ… DeterminaciÃ³n de complejidad

### ğŸ¯ CaracterÃ­sticas Clave

1. **Selectividad Inteligente**: El LLM analiza la pregunta y responde SOLO lo solicitado
2. **Contexto Conversacional**: Usa historial de 24h para hacer referencias ("como te dije antes...")
3. **AdaptaciÃ³n Emocional**: Ajusta tono segÃºn urgencia (examen maÃ±ana) o celebraciÃ³n (dÃ­a libre)
4. **Proactividad Contextual**: Sugiere informaciÃ³n Ãºtil relacionada automÃ¡ticamente
5. **VariaciÃ³n de Longitud**: Respuestas cortas para preguntas simples, detalladas para complejas
6. **Fallback Seguro**: Si LLM falla, usa templates legacy como backup

### âš ï¸ Implicaciones

**Ventajas**:
- Naturalidad mÃ¡xima en respuestas
- Mejor experiencia de usuario
- Flexibilidad total ante queries complejas
- Contexto conversacional rico

**Trade-offs**:
- Latencia: +1-2s por respuesta (total: 2-4s)
- Costo: ~$0.001-0.003 por respuesta
- Tokens: 500-1500 tokens/respuesta (vs ~50 con templates)
- Variabilidad: Menos predecible (requiere prompt engineering)

### âœ… Estado de IntegraciÃ³n

**Agents con LLM Response Generator integrado**:
- âœ… **Academic Agent** - Consultas de horarios (`_handle_schedules`)
- âœ… **Calendar Agent** - Consultas de exÃ¡menes (`_handle_exams`)

**Agents pendientes** (usando templates legacy):
- â³ Academic Agent - Inscripciones, profesores, aulas, crÃ©ditos VU
- â³ Calendar Agent - Eventos, feriados
- â³ Financial Agent - Pagos, deudas
- â³ Policies Agent - Reglamentos, syllabi

### ğŸš€ PrÃ³ximos Pasos

1. âœ… ~~MigraciÃ³n completa: Integrar LLM generator en calendar_agent.py~~ **COMPLETADO**
2. **MigraciÃ³n incremental**: Integrar en resto de mÃ©todos de academic y calendar agents
3. **OptimizaciÃ³n de prompts**: Ajustar system prompts basado en feedback de usuarios reales
4. **A/B Testing**: Comparar satisfacciÃ³n usuario con templates vs LLM
5. **Memoria de largo plazo**: Integrar Memory MCP para recordar preferencias del usuario
6. **Streaming**: Implementar streaming de tokens para mejor UX

---

**Ãšltima actualizaciÃ³n**: Sistema de Respuestas LLM implementado (Octubre 2025)
