# üéì Universidad Austral Bot - Gu√≠a de Proyecto

## üìã Descripci√≥n General

Bot universitario con arquitectura LangGraph + FastAPI + Supabase para atenci√≥n estudiantil v√≠a WhatsApp.

**Stack Tecnol√≥gico:**
- **Backend**: Python 3.11, FastAPI, LangGraph
- **Database**: Supabase (PostgreSQL)
- **LLM**: OpenAI GPT-4o-mini, Claude Opus 4.1
- **Integration**: Chatwoot (WhatsApp), n8n (tools)
- **Deployment**: Docker Compose

## üèóÔ∏è Arquitectura del Sistema

### Componentes Principales
1. **Supervisor Agent** (LangGraph): Orquesta 4 sub-agents especializados
2. **Academic Agent**: Horarios, inscripciones, profesores
3. **Calendar Agent**: Ex√°menes, fechas, calendario acad√©mico
4. **Financial Agent**: Pagos, deudas (futuro)
5. **Policies Agent**: Reglamentos, syllabi (futuro)

### Flujo de Datos
```
WhatsApp ‚Üí Chatwoot Webhook ‚Üí FastAPI ‚Üí LangGraph Supervisor
    ‚Üì
Clasificador de Query ‚Üí Agent Especializado ‚Üí n8n Tool ‚Üí Supabase
    ‚Üì
Response ‚Üí FastAPI ‚Üí Chatwoot ‚Üí WhatsApp
```

## üìÇ Estructura de Directorios

```
app/
‚îú‚îÄ‚îÄ core/              # Config, constants, exceptions
‚îú‚îÄ‚îÄ models/            # Pydantic models (CR√çTICO: usar properties)
‚îú‚îÄ‚îÄ agents/            # LangGraph agents
‚îú‚îÄ‚îÄ tools/             # n8n tool wrappers
‚îú‚îÄ‚îÄ database/          # Supabase repositories
‚îú‚îÄ‚îÄ integrations/      # Chatwoot, WhatsApp webhooks
‚îî‚îÄ‚îÄ utils/             # Logging, formatters
```

## üõ†Ô∏è Comandos Importantes

### Docker
```bash
# Rebuild completo
docker-compose down && docker-compose build --no-cache && docker-compose up -d

# Ver logs
docker-compose logs -f university-agent

# Restart solo el bot
docker-compose restart university-agent
```

### Testing
```bash
# Tests locales
pytest tests/ -v

# Test espec√≠fico
pytest tests/unit/test_agents.py -v
```

### Git
```bash
# Branch naming: feature/nombre-funcionalidad
# Commits: usar conventional commits (feat:, fix:, refactor:)
```

## üìù Convenciones de C√≥digo

### Python Style
- **Type hints**: SIEMPRE usar (mandatory)
- **Pydantic Models**: Preferir sobre dicts
- **Properties calculadas**: Usar @property en models
- **Docstrings**: Google style
- **Naming**:
  - Variables: snake_case
  - Classes: PascalCase
  - Constants: UPPER_CASE
  - Private: _leading_underscore

### Importaciones
```python
# Orden: stdlib ‚Üí third-party ‚Üí local
import os
from typing import Dict, Any

from fastapi import FastAPI
from langchain_core.messages import HumanMessage

from app.core import settings
from app.models import HorariosResponse
```

### Logs
```python
# Usar logger, no print
from app.utils.logger import get_logger
logger = get_logger(__name__)

logger.info(f"Query clasificada como: {query_type}")
logger.error(f"Error: {e}", exc_info=True)
```

## ‚ö†Ô∏è Reglas Cr√≠ticas

### NUNCA:
- ‚ùå Usar `dict` gen√©ricos - usar Pydantic Models
- ‚ùå Hacer cambios en `main.py` sin consultar (critical)
- ‚ùå Modificar `.env` en commits (usar .env.example)
- ‚ùå Hardcodear URLs o API keys
- ‚ùå Usar `print()` - usar `logger`

### SIEMPRE:
- ‚úÖ Validar UUIDs antes de queries a DB
- ‚úÖ Usar try/except en tool calls
- ‚úÖ Retornar Pydantic models desde repositories
- ‚úÖ Usar properties calculadas en models
- ‚úÖ Formatear responses con emojis (1-2 por mensaje)

## üîß Configuraci√≥n de Herramientas

### Filtrado de Contexto de Mensajes ‚ö° NUEVO
**Problema solucionado**: Reducir consumo de tokens enviando solo contexto relevante

**Configuraci√≥n** (`.env`):
```bash
MESSAGE_HISTORY_HOURS=24  # Solo mensajes de √∫ltimas N horas
```

**C√≥mo funciona**:
- `supervisor.py:_get_filtered_message_history()` filtra mensajes por timestamp
- Solo incluye mensajes de las √∫ltimas 24 horas (configurable)
- Reduce significativamente el consumo de tokens
- Mantiene contexto relevante sin informaci√≥n obsoleta

**Logs**:
```
üìä Historial filtrado para {session_id}:
   {N} mensajes √∫nicos de {M}/{T} estados
   (√∫ltimas 24h desde {timestamp})
```

### n8n Webhooks
Base URL: `https://n8n.tucbbs.com.ar/webhook`

Tools disponibles:
- `consultar_horarios`
- `ver_inscripciones`
- `buscar_profesor`
- `consultar_aula`
- `estado_cuenta`
- `consultar_creditos_vu`
- `calendario_academico`
- `consultar_examenes`
- `escalar_consulta`

### Supabase
```python
# Usar repositories, NUNCA queries directas
from app.database.academic_repository import AcademicRepository

repo = AcademicRepository()
response = await repo.get_horarios_alumno(request)
```

## üé® Terminolog√≠a Universitaria Argentina

- "Cursada" (no "curso")
- "Materia" (no "asignatura")
- "Comisi√≥n" (para grupos)
- "Final" / "Parcial" (ex√°menes)
- "Cuatrimestre" (no "semestre")
- "Carrera" (programa)
- Usar "vos" en respuestas

## üêõ Debugging

### Logs de LangSmith
- Project: `universidad-austral-bot`
- Ver traces en: https://smith.langchain.com

### Common Issues
1. **ValidationError en Pydantic**: Revisar types en Request
2. **Tool timeout**: n8n puede tardar >30s
3. **Context window full**: Usar properties en vez de full objects

## üìä Performance

- **Max response time**: 5 segundos
- **Token budget**: Usar fuzzy matching antes de LLM
- **Cache**: Memory MCP mantiene contexto

## üéØ Workflow Recomendado

1. **Plan**: Leer c√≥digo existente, entender arquitectura
2. **Research**: Buscar patrones similares en el c√≥digo
3. **Implement**: Crear/modificar c√≥digo
4. **Test**: Verificar que funciona
5. **Review**: Check logs de LangSmith

## üìö Recursos

- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **Pydantic Docs**: https://docs.pydantic.dev/
- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **Supabase Docs**: https://supabase.com/docs

---

## ü§ñ Sistema de Respuestas LLM (NUEVO - Octubre 2025)

### üéØ Objetivo

Transformar el bot de respuestas basadas en templates r√≠gidos a **respuestas generadas por LLM** que sean:
- Naturales y conversacionales
- Contextuales (usan historial de 24h)
- Selectivas (responden solo lo preguntado)
- Emp√°ticas (adaptan tono seg√∫n emoci√≥n/urgencia)
- Proactivas (sugieren informaci√≥n √∫til relacionada)

### üìä Arquitectura

**Flujo anterior (templates)**:
```
Usuario ‚Üí Agent ‚Üí Tool (datos) ‚Üí Template Formatter ‚Üí Usuario
```

**Flujo nuevo (LLM)**:
```
Usuario ‚Üí Agent ‚Üí Tool (datos) ‚Üí LLM Response Generator ‚Üí Usuario
                                         ‚Üë
                                 Context Enhancer
                                 (historial 24h + emociones + proactividad)
```

### üìÅ Nuevos M√≥dulos

#### 1. `app/models/context.py`
Modelos Pydantic para contexto conversacional enriquecido:
- `EmotionalState` - Estado emocional (tono, urgencia, empat√≠a)
- `QueryEntity` - Entidades extra√≠das (aula, horario, profesor)
- `ConversationContext` - Contexto completo (historial, emociones, sugerencias)
- `ResponseStrategy` - Estrategia de respuesta (longitud, tono, formato)

#### 2. `app/prompts/system_prompts.py`
System prompts estructurados para el LLM:
- Prompts base por agent (academic, calendar)
- Instrucciones de tono emocional (urgente, celebraci√≥n, empat√≠a, √°nimo)
- Instrucciones de longitud (short, medium, detailed, auto)
- **Instrucciones de selectividad** (CR√çTICO: responder solo lo preguntado)
- Instrucciones de proactividad y referencias hist√≥ricas

#### 3. `app/utils/llm_response_generator.py`
Generador principal de respuestas con LLM:
- `LLMResponseGenerator` class - Generador principal
- `generate_natural_response()` - Helper function para uso f√°cil
- `should_use_llm_generation()` - Determina si usar LLM vs templates
- `determine_data_complexity()` - Analiza complejidad de datos

#### 4. `app/utils/response_strategy.py`
Analiza queries y determina estrategia de respuesta:
- `QueryEntityExtractor` - Extrae entidades (aula, horario, profesor)
- `ResponseStrategyBuilder` - Construye estrategia completa
- `build_response_strategy()` - Helper function

#### 5. `app/utils/context_enhancer.py`
Enriquece contexto conversacional:
- `ContextEnhancer` class - Enriquecedor principal
- Extrae historial relevante (√∫ltimas 24h)
- Detecta estado emocional
- Genera sugerencias proactivas
- `enhance_conversation_context()` - Helper function

### ‚öôÔ∏è Configuraci√≥n (.env)

```bash
# LLM Response Generation
RESPONSE_GENERATION_MODE=llm  # "llm", "template", "hybrid"
LLM_RESPONSE_MODEL=gpt-4o-mini  # Opcional, usa LLM_MODEL si no se especifica
LLM_RESPONSE_TEMPERATURE=0.5  # Balance creatividad/precisi√≥n
MAX_RESPONSE_TOKENS=500

# Context Enhancement
ENABLE_CONTEXT_ENHANCEMENT=true
CONTEXT_LOOKBACK_HOURS=24  # Historial conversacional
ENABLE_PROACTIVE_SUGGESTIONS=true

# Response Strategy
ENABLE_SMART_FILTERING=true  # LLM filtra datos relevantes
DEFAULT_RESPONSE_LENGTH=auto  # "short", "medium", "detailed", "auto"
```

### üîÑ Integraci√≥n en Agents

**Ejemplo en `academic_agent.py`** (m√©todo `_handle_schedules`):

```python
# Obtener datos del tool
result_dict = await self.tools.consultar_horarios(params)
response = HorariosResponse(**result_dict)

# Verificar si usar LLM generation
if should_use_llm_generation():
    # Obtener sesi√≥n
    session = session_manager.get_session(session_id)

    # Enriquecer contexto
    context = await enhance_conversation_context(
        current_query=query,
        query_type="horarios",
        user_name=user_info["nombre"],
        session=session,
        data=response
    )

    # Construir estrategia
    strategy, entities = build_response_strategy(
        query=query,
        data=response,
        context=context
    )

    # Generar respuesta natural
    natural_response = await generate_natural_response(
        data=response,
        original_query=query,
        user_name=user_info["nombre"],
        query_type="horarios",
        agent_type="academic",
        context=context,
        strategy=strategy
    )

    # Actualizar contexto en sesi√≥n
    session.update_query_context(
        query=query,
        query_type="horarios",
        query_data={"materia": materia, "temporal": contexto_temporal},
        response_summary=natural_response[:100]
    )

    return natural_response
else:
    # Fallback a templates
    return self._format_schedule_response(response, nombre, contexto_temporal)
```

### üìä Comparaci√≥n de Respuestas

**Pregunta**: "¬øEn qu√© aula tengo √©tica?"

**Antes (template r√≠gido)**:
```
üìö Horarios de Juan

‚Ä¢ √âtica y Deontolog√≠a (14:00 - 16:00)
  üìç Aula R3 üè´
  üë®‚Äçüè´ Prof. Garc√≠a Mart√≠nez
  ‚è±Ô∏è Duraci√≥n: 120 minutos

¬øAlgo m√°s? ü§ù
```

**Despu√©s (LLM selectivo)**:
```
Tu clase de √âtica es en el aula R3 üìç

(Es los viernes a las 14hs con Garc√≠a Mart√≠nez)
```

**Pregunta**: "¬øCu√°ndo tengo clases ma√±ana?"

**Despu√©s (LLM con proactividad)**:
```
Ma√±ana ten√©s un d√≠a bastante cargado con 3 materias üí™

14:00 - √âtica (R3)
16:00 - Programaci√≥n I (L2)
18:30 - Matem√°tica Discreta (R1)

Ah, y acordate que ten√©s el parcial de Nativa Digital el lunes üü°
¬øQuer√©s que te recuerde el aula?
```

### ‚úÖ Testing

Ejecutar test de validaci√≥n:
```bash
python test_llm_response_system.py
```

El test valida:
1. ‚úÖ Imports de m√≥dulos nuevos
2. ‚úÖ Configuraciones cargadas correctamente
3. ‚úÖ Modelos Pydantic funcionando
4. ‚úÖ Extractores de entidades
5. ‚úÖ Generaci√≥n de prompts
6. ‚úÖ Determinaci√≥n de complejidad

### üéØ Caracter√≠sticas Clave

1. **Selectividad Inteligente**: El LLM analiza la pregunta y responde SOLO lo solicitado
2. **Contexto Conversacional**: Usa historial de 24h para hacer referencias ("como te dije antes...")
3. **Adaptaci√≥n Emocional**: Ajusta tono seg√∫n urgencia (examen ma√±ana) o celebraci√≥n (d√≠a libre)
4. **Proactividad Contextual**: Sugiere informaci√≥n √∫til relacionada autom√°ticamente
5. **Variaci√≥n de Longitud**: Respuestas cortas para preguntas simples, detalladas para complejas
6. **Fallback Seguro**: Si LLM falla, usa templates legacy como backup

### ‚ö†Ô∏è Implicaciones

**Ventajas**:
- Naturalidad m√°xima en respuestas
- Mejor experiencia de usuario
- Flexibilidad total ante queries complejas
- Contexto conversacional rico

**Trade-offs**:
- Latencia: +1-2s por respuesta (total: 2-4s)
- Costo: ~$0.001-0.003 por respuesta
- Tokens: 500-1500 tokens/respuesta (vs ~50 con templates)
- Variabilidad: Menos predecible (requiere prompt engineering)

### ‚úÖ Estado de Integraci√≥n

**Agents con LLM Response Generator integrado**:
- ‚úÖ **Academic Agent** - Consultas de horarios (`_handle_schedules`)
- ‚úÖ **Calendar Agent** - Consultas de ex√°menes (`_handle_exams`)

**Agents pendientes** (usando templates legacy):
- ‚è≥ Academic Agent - Inscripciones, profesores, aulas, cr√©ditos VU
- ‚è≥ Calendar Agent - Eventos, feriados
- ‚è≥ Financial Agent - Pagos, deudas
- ‚è≥ Policies Agent - Reglamentos, syllabi

### üöÄ Pr√≥ximos Pasos

1. ‚úÖ ~~Migraci√≥n completa: Integrar LLM generator en calendar_agent.py~~ **COMPLETADO**
2. **Migraci√≥n incremental**: Integrar en resto de m√©todos de academic y calendar agents
3. **Optimizaci√≥n de prompts**: Ajustar system prompts basado en feedback de usuarios reales
4. **A/B Testing**: Comparar satisfacci√≥n usuario con templates vs LLM
5. **Memoria de largo plazo**: Integrar Memory MCP para recordar preferencias del usuario
6. **Streaming**: Implementar streaming de tokens para mejor UX

---

**√öltima actualizaci√≥n**: Sistema de Respuestas LLM implementado (Octubre 2025)
