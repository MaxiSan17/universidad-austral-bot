# Modelo LLM
LLM_MODEL=gpt-4o
OPENAI_API_KEY=tu-openai-key-aqui
# ANTHROPIC_API_KEY=tu-anthropic-key-aqui

# Chatwoot
CHATWOOT_URL=https://app.chatwoot.com
CHATWOOT_API_TOKEN=tu-chatwoot-token-aqui
CHATWOOT_ACCOUNT_ID=tu-account-id-aqui

# n8n
N8N_WEBHOOK_URL=https://n8n.tucbbs.com.ar/webhook
N8N_API_KEY=tu-n8n-key-aqui

# Base de datos PostgreSQL
DATABASE_URL=postgresql://user:password@localhost/universidad_austral

# WhatsApp
WHATSAPP_VERIFY_TOKEN=universidad_austral_token_2025

# ================================
# MESSAGE DEBOUNCING
# ================================
# Tiempo de espera (segundos) para juntar mensajes r√°pidos consecutivos
MESSAGE_DEBOUNCE_SECONDS=2.0

# ================================
# MESSAGE HISTORY FILTERING
# ================================
# Limitar contexto de conversaci√≥n a mensajes recientes
# Solo incluir mensajes de las √∫ltimas N horas (reduce consumo de tokens)
MESSAGE_HISTORY_HOURS=24

# ================================
# LLM STREAMING
# ================================
# Habilitar streaming de tokens del LLM (recomendado para mejor UX)
LLM_STREAMING_ENABLED=true

# ================================
# LANGSMITH TRACING
# ================================
# Observabilidad y debugging de agents
# Obtener API key desde: https://smith.langchain.com/settings
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_...
LANGCHAIN_PROJECT=universidad-austral-bot
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# ================================
# LLM RESPONSE GENERATION ü§ñ NUEVO
# ================================
# Sistema de respuestas naturales generadas por LLM (vs templates r√≠gidos)

# Modo de generaci√≥n de respuestas
# - "llm": Todas las respuestas generadas por LLM (m√°xima naturalidad)
# - "template": Usar templates tradicionales (m√°s r√°pido, menos flexible)
# - "hybrid": LLM para casos complejos, templates para simples
RESPONSE_GENERATION_MODE=llm

# Modelo LLM espec√≠fico para generar respuestas (opcional, usa LLM_MODEL si no se especifica)
# LLM_RESPONSE_MODEL=gpt-4o-mini

# Temperatura para generaci√≥n de respuestas (0.5 = balance creatividad/precisi√≥n)
LLM_RESPONSE_TEMPERATURE=0.5

# M√°ximo de tokens en respuestas generadas
MAX_RESPONSE_TOKENS=500

# ================================
# CONTEXT ENHANCEMENT
# ================================
# Enriquecimiento del contexto conversacional para respuestas m√°s naturales

# Habilitar enriquecimiento de contexto (historial, emociones, proactividad)
ENABLE_CONTEXT_ENHANCEMENT=true

# Horas de historial conversacional a considerar
CONTEXT_LOOKBACK_HOURS=24

# Habilitar sugerencias proactivas ("Ah, y ten√©s examen ma√±ana...")
ENABLE_PROACTIVE_SUGGESTIONS=true

# ================================
# RESPONSE STRATEGY
# ================================
# Configuraci√≥n de c√≥mo el bot decide qu√© informaci√≥n incluir

# Filtrado inteligente: el LLM decide qu√© datos mostrar seg√∫n la pregunta
ENABLE_SMART_FILTERING=true

# Longitud por defecto de respuestas: "short", "medium", "detailed", "auto"
DEFAULT_RESPONSE_LENGTH=auto